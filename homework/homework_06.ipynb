{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dbe9d25b",
   "metadata": {},
   "source": [
    "# HW06: Word Embeddings\n",
    "\n",
    "Remember that these homework work as a completion grade. **You can <span style=\"color:red\">not</span> skip one section this homework.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b3a0596",
   "metadata": {},
   "source": [
    "**Essay Feedback**\n",
    "\n",
    "Please provide feedback to two classmates' essays on Eduflow."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ea14794",
   "metadata": {},
   "source": [
    "**Training word2vec**\n",
    "\n",
    "In this section, we train a word2vec model using gensim. We train the model on text8 (which consists of the first 90M characters of a Wikipedia dump from 2006 and is considered one of the benchmarks for evaluating language models)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "95a38d6e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'num_records': 1701,\n",
       " 'record_format': 'list of str (tokens)',\n",
       " 'file_size': 33182058,\n",
       " 'reader_code': 'https://github.com/RaRe-Technologies/gensim-data/releases/download/text8/__init__.py',\n",
       " 'license': 'not found',\n",
       " 'description': 'First 100,000,000 bytes of plain text from Wikipedia. Used for testing purposes; see wiki-english-* for proper full Wikipedia datasets.',\n",
       " 'checksum': '68799af40b6bda07dfa47a32612e5364',\n",
       " 'file_name': 'text8.gz',\n",
       " 'read_more': ['http://mattmahoney.net/dc/textdata.html'],\n",
       " 'parts': 1}"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gensim.downloader as api\n",
    "\n",
    "api.info(\"text8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0a49444c",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = api.load(\"text8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "61fa38b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "\n",
    "##TODO train a word2vec model on this dataset, only consider words which appear at least 10 times in the corpus\n",
    "model = Word2Vec(dataset,  # list of tokenized sentences\n",
    "               workers = 4, # Number of threads to run in parallel\n",
    "               vector_size = 300,  # Word vector dimensionality     \n",
    "               min_count =  10, # Minimum word count  \n",
    "               window = 5, # Context window size      \n",
    "               sample = 1e-3, # Downsample setting for frequent words\n",
    "               )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4af69360",
   "metadata": {},
   "source": [
    "**Word Similarities**\n",
    "\n",
    "gensim models provide almost all the utility you might want to wish for to perform standard word similarity tasks. They are available in the .wv (wordvectors) attribute of the model, more details could be found [here](https://radimrehurek.com/gensim/models/keyedvectors.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5cf99280",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0. kings with similarity 0.661\n",
      "1. prince with similarity 0.646\n",
      "2. throne with similarity 0.644\n",
      "3. queen with similarity 0.633\n",
      "4. regent with similarity 0.611\n",
      "5. emperor with similarity 0.609\n",
      "6. pharaoh with similarity 0.599\n",
      "7. vii with similarity 0.595\n",
      "8. sultan with similarity 0.594\n",
      "9. aragon with similarity 0.589\n"
     ]
    }
   ],
   "source": [
    "word_vectors = model.wv\n",
    "\n",
    "##TODO find the closest words to king\n",
    "results = word_vectors.most_similar(positive=['king'])\n",
    "for idx, (key, sim) in enumerate(results):\n",
    "    print(f'{idx}. \"{key}\" with similarity {sim:.3f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c30c847",
   "metadata": {},
   "source": [
    "King is to man as woman is to X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "615b6116",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0. \"queen\" with similarity 0.619\n"
     ]
    }
   ],
   "source": [
    "##TODO find the closest word for the vector \"woman\" + \"king\" - \"man\"\n",
    "results = word_vectors.most_similar(positive=['woman', 'king'], negative=['man'], topn=1)\n",
    "for idx, (key, sim) in enumerate(results):\n",
    "    print(f'{idx}. \"{key}\" with similarity {sim:.3f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2af37627",
   "metadata": {},
   "source": [
    "**Evaluate Word Similarities** \n",
    "\n",
    "One common way to evaluate word2vec models are word analogy tasks. Let's check how good our model is on one of those. We consider the [WordSim353](http://alfonseca.org/eng/research/wordsim353.html) benchmark, the task is to determine how similar two words are."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "71515b20",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('tiger', 'cat'), ('tiger', 'tiger'), ('plane', 'car')] [7.35, 10.0, 5.77]\n"
     ]
    }
   ],
   "source": [
    "#!wget http://alfonseca.org/pubs/ws353simrel.tar.gz\n",
    "#!tar xf ws353simrel.tar.gz\n",
    "\n",
    "path = \"wordsim353_sim_rel/wordsim_similarity_goldstandard.txt\"\n",
    "\n",
    "def load_data(path):\n",
    "    X, y = [], []\n",
    "    with open(path) as f:\n",
    "        for line in f:\n",
    "            line = line.strip().split(\"\\t\")\n",
    "            X.append((line[0], line[1])) # each entry in x contains two words, e.g. X[0] = (tiger, cat)\n",
    "            y.append(float(line[-1])) # each entry in y is the annotation how similar two words are, e.g. Y[0] = 7.35\n",
    "    return X, y\n",
    "\n",
    "X, y = load_data(path)\n",
    "print (X[:3], y[:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "9c8ced33",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"tiger\", \"cat\": 0.604\n",
      "\"tiger\", \"tiger\": 1.000\n",
      "\"plane\", \"car\": 0.428\n"
     ]
    }
   ],
   "source": [
    "##TODO compute how similar the pairs in the WordSim353 are according to our model\n",
    "##TODO if  aword is not present in our model, we assign similarity 0 for the respective text pair\n",
    "y_w2v = []\n",
    "for x1, x2 in X:\n",
    "    if x1 in word_vectors and x2 in word_vectors:\n",
    "        y_w2v.append(word_vectors.similarity(x1, x2))\n",
    "    else:\n",
    "        y_w2v.append(0)\n",
    "\n",
    "for (x1, x2), sim in zip(X[:3], y_w2v[:3]):\n",
    "    print(f'\"{x1}\", \"{x2}\": {sim:.3f}')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "ebd47f93",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spearman correlation=0.647 with pvalue=1.7e-25\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from scipy.stats import spearmanr\n",
    "\n",
    "##TODO compute spearman's rank correlation between our prediction and the human annotations\n",
    "sp = spearmanr(y, y_w2v)\n",
    "print(f'Spearman correlation={sp.correlation:.3f} with pvalue={sp.pvalue:.1e}')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "9ec86899",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_4642/3725190145.py:7: UserWarning: [W007] The model you're using has no word vectors loaded, so the result of the Doc.similarity method will be based on the tagger, parser and NER, which may not give useful similarity judgements. This may happen if you're using one of the small models, e.g. `en_core_web_sm`, which don't ship with word vectors and only use context-sensitive tensors. You can always add your own word vectors, or use one of the larger models instead if available.\n",
      "  y_spacy.append(en(x1).similarity(en(x2)))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"tiger\", \"cat\": 0.633\n",
      "\"tiger\", \"tiger\": 1.000\n",
      "\"plane\", \"car\": 0.713\n",
      "Spearman correlation=0.031 with pvalue=6.6e-01\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "en = spacy.load('en_core_web_sm')\n",
    "\n",
    "##TODO compute word similarities in the WordSim353 dataset using spaCy word embeddings\n",
    "y_spacy = []\n",
    "for x1, x2 in X:\n",
    "    y_spacy.append(en(x1).similarity(en(x2)))\n",
    "\n",
    "for (x1, x2), sim in zip(X[:3], y_spacy[:3]):\n",
    "    print(f'\"{x1}\", \"{x2}\": {sim:.3f}')\n",
    "\n",
    "##TODO compute spearman's rank correlation between these similarities and the human annotations\n",
    "# Don't worry if results are not too convincing for this experiment\n",
    "sp = spearmanr(y, y_spacy)\n",
    "print(f'Spearman correlation={sp.correlation:.3f} with pvalue={sp.pvalue:.1e}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d29de774",
   "metadata": {},
   "source": [
    "**Keras Embeddings**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "3927e048",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-22T21:42:21.281177Z",
     "start_time": "2022-03-22T21:42:21.208787Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>title</th>\n",
       "      <th>lead</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>20637</th>\n",
       "      <td>sci/tech</td>\n",
       "      <td>Court Bars Rubble Removal from Jerusalem Shrin...</td>\n",
       "      <td>Reuters - Israel's Supreme Court has\\temporari...</td>\n",
       "      <td>Court Bars Rubble Removal from Jerusalem Shrin...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26554</th>\n",
       "      <td>business</td>\n",
       "      <td>Federated betting on Macy #39;s name</td>\n",
       "      <td>In a widely anticipated move, Federated Depart...</td>\n",
       "      <td>Federated betting on Macy #39;s name In a wide...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42397</th>\n",
       "      <td>sport</td>\n",
       "      <td>Soccer: US Captain Reyna Injured</td>\n",
       "      <td>MANCHESTER, England - United States team capta...</td>\n",
       "      <td>Soccer: US Captain Reyna Injured MANCHESTER, E...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47069</th>\n",
       "      <td>world</td>\n",
       "      <td>Iraqi children bear brunt of bombings</td>\n",
       "      <td>Some of the children cry, refusing to speak bu...</td>\n",
       "      <td>Iraqi children bear brunt of bombings Some of ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2035</th>\n",
       "      <td>business</td>\n",
       "      <td>Confidence drops in Germany</td>\n",
       "      <td>Investor confidence in Germany, Europe #39;s b...</td>\n",
       "      <td>Confidence drops in Germany Investor confidenc...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          label                                              title  \\\n",
       "20637  sci/tech  Court Bars Rubble Removal from Jerusalem Shrin...   \n",
       "26554  business               Federated betting on Macy #39;s name   \n",
       "42397     sport                   Soccer: US Captain Reyna Injured   \n",
       "47069     world              Iraqi children bear brunt of bombings   \n",
       "2035   business                        Confidence drops in Germany   \n",
       "\n",
       "                                                    lead  \\\n",
       "20637  Reuters - Israel's Supreme Court has\\temporari...   \n",
       "26554  In a widely anticipated move, Federated Depart...   \n",
       "42397  MANCHESTER, England - United States team capta...   \n",
       "47069  Some of the children cry, refusing to speak bu...   \n",
       "2035   Investor confidence in Germany, Europe #39;s b...   \n",
       "\n",
       "                                                    text  \n",
       "20637  Court Bars Rubble Removal from Jerusalem Shrin...  \n",
       "26554  Federated betting on Macy #39;s name In a wide...  \n",
       "42397  Soccer: US Captain Reyna Injured MANCHESTER, E...  \n",
       "47069  Iraqi children bear brunt of bombings Some of ...  \n",
       "2035   Confidence drops in Germany Investor confidenc...  "
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Import the AG news dataset (same as hw01)\n",
    "#Download them from here \n",
    "# !wget https://raw.githubusercontent.com/mhjabreel/CharCnn_Keras/master/data/ag_news_csv/train.csv\n",
    "\n",
    "import pandas as pd\n",
    "import nltk\n",
    "df = pd.read_csv('train.csv')\n",
    "\n",
    "df.columns = [\"label\", \"title\", \"lead\"]\n",
    "label_map = {1:\"world\", 2:\"sport\", 3:\"business\", 4:\"sci/tech\"}\n",
    "def replace_label(x):\n",
    "\treturn label_map[x]\n",
    "df[\"label\"] = df[\"label\"].apply(replace_label) \n",
    "df[\"text\"] = df[\"title\"] + \" \" + df[\"lead\"]\n",
    "df = df.sample(n=10000) # # only use 10K datapoints\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "a49d6b6e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-22T21:40:20.385383Z",
     "start_time": "2022-03-22T21:40:18.447956Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20637    [court, bars, rubble, removal, from, jerusalem...\n",
      "26554    [federated, betting, on, macy, 39, s, name, in...\n",
      "42397    [soccer, us, captain, reyna, injured, manchest...\n",
      "47069    [iraqi, children, bear, brunt, of, bombings, s...\n",
      "2035     [confidence, drops, in, germany, investor, con...\n",
      "Name: toks, dtype: object\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.text import text_to_word_sequence\n",
    "\n",
    "##TODO tokenize the text using text_to_word_sequence\n",
    "df['toks'] = df['text'].apply(text_to_word_sequence)\n",
    "print(df['toks'].head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "c4c0f840",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-22T21:40:23.322875Z",
     "start_time": "2022-03-22T21:40:23.311923Z"
    }
   },
   "outputs": [],
   "source": [
    "from keras.preprocessing.text import one_hot\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "length_vocab = 1000\n",
    "max_seq_length = 100\n",
    "\n",
    "#TODO create a one_hot representation for each word and truncate/pad the sequences such that they are all of the same length\n",
    "X = df['toks'].apply(lambda toks: [one_hot(t, length_vocab) for t in toks])\n",
    "X = X.apply(lambda seq: pad_sequences(seq, max_seq_length))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "c3d193dd",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-22T21:40:28.364553Z",
     "start_time": "2022-03-22T21:40:28.354695Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding_1 (Embedding)     (None, 100, 100)          100000    \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 100,000\n",
      "Trainable params: 100,000\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Embedding\n",
    "\n",
    "##TODO create a sequential model with just one embedding layer and show the model summary\n",
    "model = Sequential()                        # create a sequential model\n",
    "model.add(Embedding(\n",
    "    input_dim=length_vocab,                 # one hot input dimension \n",
    "    output_dim=100,                         # embedding dimension\n",
    "    input_length=max_seq_length             # sequence length\n",
    "))\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cac3bd2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
